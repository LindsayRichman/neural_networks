# Designing Your Neural Networks:
## A Step by Step Walkthrough

What’s a good learning rate? How many hidden layers should your network have? Is dropout actually useful? Why are your gradients vanishing?

In this notebook we'll peel the curtain behind some of the more confusing aspects of neural nets, and help you make smart decisions about your neural network architecture.

We’ll also see how we can use Weights and Biases inside Kaggle kernels to monitor performance and pick the best architecture for our neural network!

**I highly recommend forking this notebook and playing with the different building blocks to hone your intuition.**

I made a quick **demo** to walk you through this kernel: https://www.loom.com/share/fb64035e4576467489cf0f2ad9cff92a.

## Slides
If you found this repo via one of my talks, here's a link to the **[slides.](https://www.slideshare.net/LavanyaShukla/designing-your-neural-networks-a-step-by-step-walkthrough/)**

## Get In Touch
If you have any more questions or feedback, please don't hesitate to [message me](https://twitter.com/lavanyaai)!
